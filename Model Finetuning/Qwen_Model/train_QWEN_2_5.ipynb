{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YAO08Jsq6q3M"
   },
   "source": [
    "# Fine-tune QWEN3 on Multilingual Jokes\n",
    "\n",
    "This notebook fine-tunes the `Qwen/Qwen3-4B-Instruct` model on a multilingual jokes dataset (English, Spanish, Chinese).\n",
    "\n",
    "**Instructions:**\n",
    "1. Upload `train.jsonl` and `val.jsonl` to the Colab files area (left sidebar).\n",
    "2. Run all cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4mNgg5E8YviI"
   },
   "source": [
    "1. Installation and Google Drive Setup\n",
    "Include the necessary libraries for 4-bit quantization (QLoRA) and mount Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F1KQNF2I6q3Q",
    "outputId": "5a94ae3d-2fa7-4c67-c476-5fba4e489317"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Install advanced fine-tuning and quantization libraries\n",
    "!pip install -U -q --no-cache-dir transformers datasets accelerate peft bitsandbytes sentencepiece trl\n",
    "\n",
    "# Mount Google Drive for persistent storage\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q8WLZprlkLFF",
    "outputId": "44b08185-41dd-4422-d111-1b9c64a1e7c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flash_attn\n",
      "  Downloading flash_attn-2.8.3.tar.gz (8.4 MB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/8.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/8.4 MB\u001b[0m \u001b[31m116.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m161.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from flash_attn) (2.9.0+cu126)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from flash_attn) (0.8.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->flash_attn) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->flash_attn) (3.0.3)\n",
      "Building wheels for collected packages: flash_attn\n",
      "  Building wheel for flash_attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for flash_attn: filename=flash_attn-2.8.3-cp312-cp312-linux_x86_64.whl size=253780426 sha256=4e2f9e39313266b1544b68138b15b91ee6221eccf14f7902b7c6620351340810\n",
      "  Stored in directory: /root/.cache/pip/wheels/3d/59/46/f282c12c73dd4bb3c2e3fe199f1a0d0f8cec06df0cccfeee27\n",
      "Successfully built flash_attn\n",
      "Installing collected packages: flash_attn\n",
      "Successfully installed flash_attn-2.8.3\n"
     ]
    }
   ],
   "source": [
    "!pip install flash_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YOdS1cAoYmDm"
   },
   "source": [
    "2. Configure Model and Quantization.\n",
    "\n",
    "We use 4-bit quantization (NF4) so that the 80B model fits into 80GB VRAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 557,
     "referenced_widgets": [
      "0b161c691f5e4ec8b250c470ccd5b6dc",
      "eb1ff576b6204bf3bbec01b0c89a8bcf",
      "88c46922672c413fab225ceef8aea073",
      "416ead483d29422bb6e2d6397b8832be",
      "cc48ce7c4db5446884be7c8ada03f040",
      "508b3ebfb0d943808940cd92643ebc58",
      "265da87ea1a544e898e8d4ffe1e51540",
      "41ca6ad2bdd341f9bc3900d8cc8285ed",
      "730738cb014a43e48d301fd7a2f3c41f",
      "416d0a44394744ab9dc9e35ea97a3838",
      "9394a45cad464964913efa4aa45e8062",
      "779d562c76b645f6ba47e75b62aa032f",
      "c7d890bee2284dc596f2bb8bdc2893d8",
      "fda880a7d1344583991bfd7fad071dbf",
      "3dfafb72e94a4247ac21eb3dd5e77611",
      "a2b282a1571540b9831bbaef3ae4bdd6",
      "9fc53345da004a74987387aeef4726c3",
      "8686947613c049218e91480765338eeb",
      "4c53a6dd569843c0aa5fe99a132312bd",
      "1954841d52834ed798a46c39c361341b",
      "d18c649214554b44b381a3ece8e8afec",
      "788de656b5ab490596851623ca5f173d",
      "01bb146c0e864bef86baebc9dfd9a4b1",
      "f34923835f6b46f3b797d88c21aaad9f",
      "4e1090abeedf42d28ec02cb3a4256cc6",
      "a13c22fa8db2449384e2df1b7e1c28b4",
      "78eff53ec77f43d6a92e854bf508b467",
      "43265a9ba95641249c24c9ad2cfe04c5",
      "d1dff932ea2e4a9c8eca76921e8013a7",
      "142ad2d6432b4530959af04f7d3899b2",
      "8788726ca8dd4cb8a7cfcc143a065263",
      "f7214d4947d74f4f95a033b019080f3d",
      "58f871d92da44e6db85bc933d517b166",
      "8e666347b48b4266b12d2108a0edbe43",
      "c7b7839d4497467a8a33d56ef6f6f47f",
      "a13b5b4ba51b40bf8f94f3bf59bf21b8",
      "a3529d2269ac4f9bb3dd4359ad4a6d67",
      "0860f387b8424277ad40bfc180f12285",
      "04b24d704fb340b388f44d11a1d28d01",
      "54d2e15abdc34a8abe07a6a5c4030c0a",
      "85059564a1de4e02a25105b47b62b405",
      "6c7dc6568d1341e4b0784ede91390572",
      "7fad0bd48e874996937075e4de5cd3b6",
      "bc6b6e8e6623403bb42d6fdd961dc80e",
      "fcd420bdac874e328242db6851ea8842",
      "f4c054eeed2f46ae9ef35295befe0fd8",
      "3ee3cd91fcbc42399ec653ac9e719cab",
      "a70a390306d04ee09d68f5c5f8187ba5",
      "72671533640e45bda90d0cdfb6535dc2",
      "d58d27968f1740d7958369b20575bacd",
      "32148f0b21ad404ea1a7a96512055107",
      "29f3a8a93263406c90591676b160f675",
      "4f28e31ac0744d76af46adcda55f1952",
      "db6e7e59b4ef4e9fad53e3bd15797d4c",
      "2792ebc3c62345948e628caa00f96a95",
      "69c275e4da6d44829cfa48c22cc2a6b7",
      "f6172635b4a943a0b6708c27fdd4b872",
      "1153263c910246e5957dcec9414730f0",
      "e736a91dad71473fae3f46cd1eae100f",
      "400c5d2c275643569cfedffac7055037",
      "559ea4a42f8446cc939c3aacaf32a8d6",
      "4dafc78ce53f46a3a5a742d3628a96cf",
      "fef04c3d93b64e359bb28dc9733e561a",
      "c894679a43ad464892a26ec8cc4830fc",
      "4da937ea8d90420c8afade5bbea4db30",
      "44dd6f3ceab5426ab26a4628a173cb64",
      "9034f02c915f49f1aa36efc0c6b410dd",
      "1a94e72daffd43fa82e5e870d01dc683",
      "462942bd046d4e22ac2c6fc16596c5ee",
      "1381a63cc5ba4d42a06083bdc6ddbc2a",
      "297940407ec242578057dacba62eea62",
      "6d3a2f1f944d40d2b1dc756753866fb8",
      "26ac99435533471c8cbe5543b0fdc5ff",
      "1be4548bffae4432b46c0813149c42a1",
      "c66357a7d90b4adabbc9b0c68bb6f124",
      "59d18941e24b4aefbb1600f0a6c92041",
      "e5c7ad1684ed42919d670949e578cca7",
      "8028ab9a042b49018c001bbd9d899f5b",
      "733965a47b8c428386e1f12e877d5100",
      "d3bbcc4165194274b1db08c49f4de3b8",
      "b9d02d3d4cf44275abe0d64c98fbf18a",
      "3496f809e43149d480047271f01b9ef0",
      "5d00ef49ab344cb5aa84b958de3f265a",
      "8c3bf1c7cae0477283e503d8d985fe90",
      "9d64d09240a94bf981c35410f7be835e",
      "0f726dd4edbc4314b62819a765a92d1f",
      "8af4605158cf4e9dacea9d483a0778bc",
      "ef2162425147443ea4bb7c7e85f5cb4c",
      "625f722e2fa240528c23cf78f9ca7a23",
      "4b34bbcbb2b14e1cb79cedcb86772852",
      "743a62375c1d4c2a946dd7020c7ea36a",
      "84c32e295c3143f093c3701741648989",
      "ca10ec53aa5f464894079e639971fe7c",
      "0089037760444ee4a3559206e8af7ba5",
      "adb12b34124d4acaa860e608e15792d6",
      "01a707e81bf6466db508b8e139c6ef7a",
      "7beb32df21d14ba987c7b30d65b6e3b8",
      "a262745443774c8c8746873f55255478",
      "887b9ac96d044d9e9b4c474a473b6375",
      "6b3299dc86bd45b9bced843e361b0777",
      "bf2c8d4a89d04a2bbd2eeb37d2de1685",
      "e05960cfd8584ef39d4a3ffa4f4669c6",
      "423673313f1146b584f165fba8f7a32c",
      "c5a790ae528c41c0ad06555070f16368",
      "3a4cbc7fce4844c586e1e61c27b320e4",
      "f4783a35d07b49cd853c31c7d01b2334",
      "267ae0916de34dfe8b4ebfa2b564f060",
      "2a783c9a2d7840fdadd5c2732e679708",
      "91b1b29acd6742419317eb5d060f9c90",
      "03cf79ad5bd04067813a6bc20ca8a181",
      "57636f42210e42959665e79755031f8e",
      "e75cc611f58944a9ab3fa11446a9b05a",
      "b545a2b867fa46f3af290f3e9eb1283d",
      "2fc6cd699d954fc5978089f1d2bbe2ac",
      "382e5c8ecc51489aac7403672cff371d",
      "be826cd375e2413b86f7395cfa2d294c",
      "17a464aec65f415491d378b6c9f8f332",
      "2e5883685f32437db28eb7b56eca4a6b",
      "8976d6c99c6c4e1db9c3bc918cabc83e",
      "bcbee6709bf34172a9a04fd74fcc1460",
      "66b48fa817ae4faabc5677a643f10e39",
      "b6ef07de085d43039010a38d03d29895",
      "baaa11ff900e4230a02069801458b5d5",
      "f533da317acd4fa8b9c4e3319e40afe7",
      "557c4dfe3f2e4d71bef4012f93a0ee08",
      "c8056d51492848169a93693fdf01a918",
      "66afc050db7d4210964dccd621841bcd",
      "e8cad24053c943adac4a8e29b4e14cb7",
      "b47ff55048fd48adac603ceb346684b6",
      "1da7e4f18e254732b1e71667ee858ecc",
      "b4eb557a54cf49d1a179fe641627fce4",
      "f7717dbf9dc441658b69d037d90712f0",
      "18ea607f03d1409bb2048c02e40b4800",
      "6a43e09afd7a409b8d77ee167e88c44d",
      "48856006293d475da30949c770225f33",
      "a3906f611e38471fa8e765222a0a639c",
      "adc3620767c94a9fb6032ffd4444fd59",
      "9dfba16fc2834a848f87325abae01de6",
      "59c7a32e0ff940d2bde6ae4dd8fcda6e",
      "ea0683a0344a4784a8b5d1d17013089d",
      "44738209b34f4490bc49ca04ef1fde1e",
      "267c4d50c8c5484c94be84896399b688",
      "80daf68f54d64019a5ce13b2f78c380e"
     ]
    },
    "id": "_d-MPnwD6q3T",
    "outputId": "f97cf5cb-936f-43d9-b34d-e1c985d2abf3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b161c691f5e4ec8b250c470ccd5b6dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "779d562c76b645f6ba47e75b62aa032f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01bb146c0e864bef86baebc9dfd9a4b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e666347b48b4266b12d2108a0edbe43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcd420bdac874e328242db6851ea8842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69c275e4da6d44829cfa48c22cc2a6b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9034f02c915f49f1aa36efc0c6b410dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8028ab9a042b49018c001bbd9d899f5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "625f722e2fa240528c23cf78f9ca7a23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b3299dc86bd45b9bced843e361b0777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57636f42210e42959665e79755031f8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6ef07de085d43039010a38d03d29895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18ea607f03d1409bb2048c02e40b4800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "model_id = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "drive_output_dir = \"/content/drive/MyDrive/qwen2.5_7B_jokes\"\n",
    "os.makedirs(drive_output_dir, exist_ok=True)\n",
    "\n",
    "# BitsAndBytes config for 80GB VRAM efficiency\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F7ZDKUmpbbmv"
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Preprocessing logic for instruction/input/output\n",
    "def format_instruction(example):\n",
    "    user_prompt = example['instruction']\n",
    "    if example.get('input'):\n",
    "        user_prompt += f\"\\nInput: {example['input']}\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "        {\"role\": \"assistant\", \"content\": example['output']}\n",
    "    ]\n",
    "    return {\"text\": tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)}\n",
    "\n",
    "# Load datasets\n",
    "dataset = load_dataset(\"json\", data_files={\"train\": \"train.jsonl\", \"validation\": \"val.jsonl\"})\n",
    "dataset = dataset.map(format_instruction)\n",
    "\n",
    "# Optimized Budget Settings (~30-50 Compute Units)\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=drive_output_dir,\n",
    "    dataset_text_field=\"text\",\n",
    "    packing=False,                     # Disabled to avoid errors\n",
    "    max_steps=3000,                    # Limit steps to fit budget\n",
    "    per_device_train_batch_size=8,     # Good for 7B model\n",
    "    gradient_accumulation_steps=4,     # Effective batch size = 32\n",
    "    learning_rate=5e-5,\n",
    "    bf16=True,\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    gradient_checkpointing=True,\n",
    "    # max_seq_length removed to prevent TypeError\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    peft_config=lora_config,\n",
    "    processing_class=tokenizer,\n",
    "    args=sft_config,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "id": "jL3zPNlAAD3t",
    "outputId": "23117a5d-8177-458e-be6d-eefd51bd13fe"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3000/3000 2:10:42, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.898700</td>\n",
       "      <td>1.759367</td>\n",
       "      <td>1.772736</td>\n",
       "      <td>2159605.000000</td>\n",
       "      <td>0.672416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.904200</td>\n",
       "      <td>1.757211</td>\n",
       "      <td>1.765666</td>\n",
       "      <td>4028876.000000</td>\n",
       "      <td>0.672715</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m URL not available in offline run\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3000, training_loss=0.6264665171305338, metrics={'train_runtime': 7846.6412, 'train_samples_per_second': 12.235, 'train_steps_per_second': 0.382, 'total_flos': 1.4367095942483558e+18, 'train_loss': 0.6264665171305338, 'epoch': 0.11745705476435178})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Resume from the latest checkpoint automatically\n",
    "trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xH5Bo0Hm6q3a"
   },
   "source": [
    "## Inference / Testing\n",
    "Let's test the fine-tuned model with some prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sCsKC52e6q3b",
    "outputId": "e8d0b6f0-fb46-4b82-ece0-8ee58f992bc1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting fp32 inputs back to torch.float16 for flash-attn compatibility.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- English ---\n",
      "Doctor: \"So you've been eating one apple a day\" Patient: \"Yes, and I'm still alive.\" Doctor: \"Well that's good news.\" Patient: \"I know. That's why I'm going back to my doctor.\"\n",
      "\n",
      "--- Spanish ---\n",
      "- ¿Qué dice el médico al darle una manzana a un paciente?\n",
      "- ¡Una!\n",
      "- ¿Y si no se la come?\n",
      "- ¡Dos!\n",
      "\n",
      "--- Chinese ---\n",
      "程序员:我用Java写了一个程序。 朋友:你为什么不用Python呢? 程序员:因为我的女朋友是C++\n"
     ]
    }
   ],
   "source": [
    "def generate_joke(instruction):\n",
    "    # Standard conversational format for Qwen3-Instruct\n",
    "    messages = [{\"role\": \"user\", \"content\": instruction}]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=True,\n",
    "            temperature=0.7, # Recommended for Qwen3 creative tasks\n",
    "            top_p=0.8,       # Recommended for Qwen3\n",
    "            top_k=20,        # Recommended for Qwen3\n",
    "            repetition_penalty=1.1\n",
    "        )\n",
    "\n",
    "    # Correctly trim the prompt from the output\n",
    "    output_ids = generated_ids[0][len(inputs.input_ids[0]):]\n",
    "    return tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "# --- Your Requested Test Cases ---\n",
    "print(\"--- English ---\")\n",
    "print(generate_joke(\"Write a joke containing the following words: doctor, apple\"))\n",
    "\n",
    "print(\"\\n--- Spanish ---\")\n",
    "print(generate_joke(\"Escribe un chiste que contenga las siguientes palabras: médico, manzana\"))\n",
    "\n",
    "print(\"\\n--- Chinese ---\")\n",
    "print(generate_joke(\"讲一个程序员的笑话\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QcYzFcw51oZL",
    "outputId": "540fea04-42ae-4552-fe48-54d74559d182"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's a star's favorite kind of berry? A raisin!\n"
     ]
    }
   ],
   "source": [
    "print(generate_joke(\"Write a joke containing the following words: star, berry\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ekxEP5gs0lKz",
    "outputId": "99801c62-9959-4084-8c44-f6260bb02e4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Extended Quality Testing ---\n",
      "\n",
      "Test 1:\n",
      "What did the astronaut say when he ate his last sandwich? \"I don't want to go back there again.\"\n",
      "------------------------------\n",
      "\n",
      "Test 2:\n",
      "- Mamá, mamá, hoy en la escuela me han puesto invisible.\n",
      "- ¡No es posible! ¿Cómo se te ha puesto invisible?\n",
      "- Mira... yo estaba jugando al fútbol y cuando el portero salió del área, me puse detrás de él...\n",
      "------------------------------\n",
      "\n",
      "Test 3:\n",
      "我那钱都花了。|啊？都花完了?拿什么来请客呀?\n",
      "------------------------------\n",
      "\n",
      "Test 4:\n",
      "What do you call a banana in a suit? A peeling\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "more_test_cases = [\n",
    "    \"Write a joke containing the following words: astronaut, sandwich.\",\n",
    "    \"Escribe un chiste de 'Mamá, mamá' que contenga las palabras: escuela, invisible.\",\n",
    "    \"给定一段相声台词，请从多个备选项中选择最合适的逗哏回复。\\nInput: 捧哏：你这人怎么回事？说好了请客，怎么兜里一分钱没有？\",\n",
    "    \"Write a joke using the word 'banana' exactly three times.\"\n",
    "]\n",
    "\n",
    "print(\"--- Extended Quality Testing ---\")\n",
    "for i, prompt in enumerate(more_test_cases):\n",
    "    print(f\"\\nTest {i+1}:\")\n",
    "    print(generate_joke(prompt))\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5UGay0XF3-xG",
    "outputId": "b165f728-c2f3-4c47-b863-161f739e430a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- English: Dog & Homework ---\n",
      "Why did the dog do his homework on the floor? Because he was a floor hound!\n",
      "\n",
      "--- English: Astronaut & Sandwich ---\n",
      "What do astronauts call their sandwiches? Space hamwiches\n",
      "\n",
      "--- English: Vampire & Mirror ---\n",
      "What do vampires and mirrors have in common? They both hate themselves!\n",
      "\n",
      "--- English: Wifi & Island ---\n",
      "What did the wifi say to the island? \"I'm not so good at this.\"\n",
      "\n",
      "--- Spanish: Beach & Winter (Playa y Invierno) ---\n",
      "- ¿Qué hace una chica de la playa en el invierno?\n",
      "- Se pone unas gafas de sol y se va al cine.\n",
      "\n",
      "--- Spanish: Elephant & Fridge (Elefante y Nevera) ---\n",
      "- ¿Qué es lo más parecido al elefante que puedes encontrar en una nevera?\n",
      "- El cebiche.\n",
      "\n",
      "--- Spanish: Waiter & Fly (Camarero y Mosca) ---\n",
      "- ¿Qué le pongo al señor?\n",
      "- ¡Un vino! \n",
      "- Pero no hay en el menú...\n",
      "- ¡No se preocupe, camarero!\n",
      "- ¡Que ya estoy aquí!\n",
      "\n",
      "--- Spanish: Ghost & Sheet (Fantasma y Sábana) ---\n",
      "- ¿Por qué no salen los fantasmas de la cama?\n",
      "- Porque tienen miedo de la sábana.\n",
      "\n",
      "--- Chinese: Boss & Salary (老板 & 工资) ---\n",
      "今天我被老板辞退了。 他跟我说:“你工作太差劲了,你的工资比我的还高。” 我说:“那我给你提个建议吧,你把工资给我,我来工作!”\n",
      "\n",
      "--- Chinese: Phone & Toilet (手机 & 厕所) ---\n",
      "我今天在厕所里玩手机,突然发现我的屁股上长了一只小鸡。 这个世界真奇妙!\n",
      "\n",
      "--- Chinese: Rabbit & Turtle (兔子 & 乌龟) ---\n",
      "兔子和乌龟赛跑。 兔子先到终点了,然后又跑回去接乌龟,结果还是输给了乌龟。\n",
      "\n",
      "--- Chinese: Dumpling & Vinegar (饺子 & 醋) ---\n",
      "我跟女朋友说:“你以后要是想吃饺子,就拿点醋来,我给你包。” 她却回答道:“你想吃醋吗?\n"
     ]
    }
   ],
   "source": [
    "# --- English Word Pairs ---\n",
    "print(\"\\n--- English: Dog & Homework ---\")\n",
    "print(generate_joke(\"Write a joke containing the following words: dog, homework\"))\n",
    "\n",
    "print(\"\\n--- English: Astronaut & Sandwich ---\")\n",
    "print(generate_joke(\"Write a joke containing the following words: astronaut, sandwich\"))\n",
    "\n",
    "print(\"\\n--- English: Vampire & Mirror ---\")\n",
    "print(generate_joke(\"Write a joke containing the following words: vampire, mirror\"))\n",
    "\n",
    "print(\"\\n--- English: Wifi & Island ---\")\n",
    "print(generate_joke(\"Write a joke containing the following words: wifi, island\"))\n",
    "\n",
    "\n",
    "# --- Spanish Word Pairs ---\n",
    "print(\"\\n--- Spanish: Beach & Winter (Playa y Invierno) ---\")\n",
    "print(generate_joke(\"Escribe un chiste que contenga las siguientes palabras: playa, invierno\"))\n",
    "\n",
    "print(\"\\n--- Spanish: Elephant & Fridge (Elefante y Nevera) ---\")\n",
    "print(generate_joke(\"Escribe un chiste que contenga las siguientes palabras: elefante, nevera\"))\n",
    "\n",
    "print(\"\\n--- Spanish: Waiter & Fly (Camarero y Mosca) ---\")\n",
    "print(generate_joke(\"Escribe un chiste que contenga las siguientes palabras: camarero, mosca\"))\n",
    "\n",
    "print(\"\\n--- Spanish: Ghost & Sheet (Fantasma y Sábana) ---\")\n",
    "print(generate_joke(\"Escribe un chiste que contenga las siguientes palabras: fantasma, sábana\"))\n",
    "\n",
    "\n",
    "# --- Chinese Word Pairs ---\n",
    "print(\"\\n--- Chinese: Boss & Salary (老板 & 工资) ---\")\n",
    "print(generate_joke(\"讲一个包含以下词语的笑话：老板，工资\"))\n",
    "\n",
    "print(\"\\n--- Chinese: Phone & Toilet (手机 & 厕所) ---\")\n",
    "print(generate_joke(\"讲一个包含以下词语的笑话：手机，厕所\"))\n",
    "\n",
    "print(\"\\n--- Chinese: Rabbit & Turtle (兔子 & 乌龟) ---\")\n",
    "print(generate_joke(\"讲一个包含以下词语的笑话：兔子，乌龟\"))\n",
    "\n",
    "print(\"\\n--- Chinese: Dumpling & Vinegar (饺子 & 醋) ---\")\n",
    "print(generate_joke(\"讲一个包含以下词语的笑话：饺子，醋\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5u0zymh54tLA",
    "outputId": "63f91f5b-dc38-4984-95ba-52b4ac6ef66e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- English: Skeleton & Party ---\n",
      "Skele-tons throw the best parties. They have no skeletons to worry about!\n",
      "\n",
      "--- English: Math & Problems ---\n",
      "I don't do math problems anymore because I have to do math problems all day at work.\n",
      "\n",
      "--- English: Scarecrow & Award ---\n",
      "The Scarecrow won an Oscar for Best Costume. He wore his hat backwards.\n",
      "\n",
      "--- English: Tomato & Ketchup ---\n",
      "What did the tomato say to the ketchup? I'm not into that.\n",
      "\n",
      "--- English: Pirate & Alphabet ---\n",
      "What did the pirate say when he was looking for the letter B in the alphabet? Arrrrr!\n",
      "\n",
      "--- English: Chef & Salt ---\n",
      "Why did the chef put salt on his plate? He wanted to eat like a pretzel.\n",
      "\n",
      "--- English: Computer & Window ---\n",
      "Why do computers have so many windows? Because they're afraid of the rain.\n",
      "\n",
      "--- English: Elevator & Songs ---\n",
      "I like it when the elevators play slow songs so I can think of ways to get out.\n",
      "\n",
      "--- English: Library & Loud ---\n",
      "Library is so quiet I can hear my brain think.\n",
      "\n",
      "--- English: Gym & Pizza ---\n",
      "Gym: \"Pizza time!\" Me: \"I'm in a hurry.\" Gym: \"No problem, just run to the end of the hall and back\"\n",
      "\n",
      "--- Spanish: Moon & Cheese (Luna y Queso) ---\n",
      "- ¿Cómo es la luna?\n",
      "- Como el queso.\n",
      "- ¿Y qué tal la tierra?\n",
      "- Como una cajita de leche en polvo.\n",
      "\n",
      "--- Spanish: Clock & Time (Reloj y Tiempo) ---\n",
      "- ¿Cómo es tu nuevo reloj?\n",
      "- ¡Es de oro! \n",
      "- Bueno, pero dime ¿Cuántas horas lleva? \n",
      "- ¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡\n",
      "\n",
      "--- Spanish: Horse & Chair (Caballo y Silla) ---\n",
      "- ¿Qué le dices al caballo que tiene una silla?\n",
      "- ¡No te pongas en mi lugar!\n",
      "\n",
      "--- Spanish: Shoe & Stone (Zapato y Piedra) ---\n",
      "- ¿Cómo es el zapato del diablo?\n",
      "- No sé... no lo he probado aún.\n",
      "- ¡Qué! ¿No has probado tus zapatos?\n",
      "- Sí, pero los de la maldición.\n",
      "\n",
      "--- Spanish: Teacher & Exam (Maestro y Examen) ---\n",
      "- Maestro, ¿por qué no puedo pasar de curso?\n",
      "- Porque has estado mal en los exámenes.\n",
      "- Pero si yo he estudiado mucho...\n",
      "- No es eso, es que tú siempre te pasas el tiempo comiendo...\n",
      "\n",
      "--- Spanish: Drunk & Street (Borracho y Calle) ---\n",
      "-¿Cómo te fue en el trabajo? \n",
      "-Borrachito. \n",
      "-Pues eso es lo que me dijeron cuando salí de la calle.\n",
      "\n",
      "--- Spanish: Tomato & Road (Tomate y Carretera) ---\n",
      "Un policía le dice al conductor de un coche:\n",
      "- ¡Conduzca con más cuidado! Si el tomate no está en la carretera no puede pasarle nada.\n",
      "- ¿No es verdad? -dice el conductor-. Yo soy un hombre muy fuerte...\n",
      "\n",
      "--- Spanish: Book & WiFi (Libro y WiFi) ---\n",
      "- ¿Qué lees?\n",
      "- Los libros de los teléfonos móviles.\n",
      "- ¿Y por qué?\n",
      "- Porque tengo WiFi.\n",
      "\n",
      "--- Spanish: Bird & Cage (Pájaro y Jaula) ---\n",
      "- ¿Cuánto le pides por tu pájaro?\n",
      "- Unos 10 euros.\n",
      "- Pues vete al mercado y ve vendiéndolo. En una semana lo tendrás pagado.\n",
      "- ¿Y si no me compran? \n",
      "- Bueno... entonces te la puedes llevar a casa.\n",
      "\n",
      "--- Spanish: Doctor & Apple (Médico y Manzana) ---\n",
      "Un médico se encontró con una paciente muy nerviosa.\n",
      "- ¿Qué le pasa?\n",
      "- Me he comido una manzana y no puedo vomitarla. \n",
      "- No te preocupes, en 15 minutos estará fuera de tu cuerpo.\n",
      "\n",
      "--- Chinese: Programmer & Hair (程序员 & 头发) ---\n",
      "程序员和老板在讨论新项目。 老板:“我们的客户需要什么?” 程序员:“他们想要一些头发。” 老板:“哦?他们需要我们帮他们洗头吗?” 程序员:“不,他们想要的是一个有头发的人!”\n",
      "\n",
      "--- Chinese: Mosquito & Sleep (蚊子 & 睡觉) ---\n",
      "蚊子飞来飞去,打扰我睡觉。 我就把它拍死了。 于是它又飞回来了,继续打扰我睡觉。 我又把它拍死了。 于是我睡不着了。\n",
      "\n",
      "--- Chinese: Husband & Wallet (老公 & 钱包) ---\n",
      "今天我老公出去了,我一个人在家。 我拿出了他钱包,打开来,发现里面有一张纸条,写着: 亲爱的,我走了,你好好照顾自己!\n",
      "\n",
      "--- Chinese: Subway & Crowded (地铁 & 拥挤) ---\n",
      "昨天挤地铁。 有个哥们儿在旁边站着。 车到站了。我冲他喊:“快上来!” “你先上吧” “你先上吧” “你先上吧” “你先上吧” ... 最后我上去的时候才发现他是聋子。\n",
      "\n",
      "--- Chinese: Beef & Lamppost (牛肉 & 电线杆) ---\n",
      "我最近在学牛。 有一天我在路上走,突然看到一头大牛站在电线杆上,我就问它:“你在干嘛?” 它说:“我在等车。” 我又问:“你等多久了?” “半小时” “那你刚才干嘛站着不动啊?” “我怕车子会撞到我”\n",
      "\n",
      "--- Chinese: Student & Homework (学生 & 作业) ---\n",
      "“老师,我昨晚写作业到凌晨两点,您让我再交一份。” “你把那份拿过来吧,我看看是不是真的。” “好啊,我这就拿来。不过您别告诉我爸我妈了,不然他们又要骂我了。”\n",
      "\n",
      "--- Chinese: Fish & Water (鱼 & 水) ---\n",
      "鱼儿在水中自由地游来游去。 水中没有障碍物,鱼儿可以尽情享受生活的快乐。 有一天,鱼儿突然想:要是我能像鸟儿一样飞起来该有多好啊! 哎呀,不好了!我不能飞起来吗?\n",
      "\n",
      "--- Chinese: Clock & Late (闹钟 & 迟到) ---\n",
      "今天早上闹钟没响,我迟到了。 上班的路上,看见一个卖早点的小摊子。 旁边有个大爷在等车。 我想买点早餐,顺带给他带一份。 可是他却说:“不用了,我们家人都吃过了。” “哦,那你们吃什么?” “我们吃面条。” “什么?面条?你刚才还说吃过了啊!” “我们吃过了啊!不过我们家里的面条是用豆浆做的。”\n",
      "\n",
      "--- Chinese: Driver & Police (司机 & 警察) ---\n",
      "今天在路上被警察拦了。 我说:“你不是在执行公务吗?” 警察说:“是啊。” 我说:“那我问你一个问题吧!” 警察说:“你说。” 我说:“你们是司机还是警察呀?” 警察说:“司机” 我说:“那你就别管我了,我是警察。” 警察说:“那你为什么还要开车呢?” 我说:“我是司机啊。”\n",
      "\n",
      "--- Chinese: Money & Happiness (钱 & 快乐) ---\n",
      "我:“爸,你把钱给我吧。” 爸:“你要钱干什么?” 我:“我要买快乐!” 爸:“你真会说话!好,给你。” 于是,我拿着钱跑出去了。 结果回来的时候,我爸问我:“怎么没买快乐啊?” 我:“钱都买了糖了,还剩一毛五呢” 爸:“糖是甜的,不是快乐吗?” 我:“那你说糖是什么味儿的?” 爸:“甜呗!” 我:“那你怎么知道糖是甜的?\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# ENGLISH WORD PAIRS\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n--- English: Skeleton & Party ---\")\n",
    "print(generate_joke(\"Write a joke containing the following words: skeleton, party\"))\n",
    "\n",
    "print(\"\\n--- English: Math & Problems ---\")\n",
    "print(generate_joke(\"Write a joke containing the following words: math, problems\"))\n",
    "\n",
    "print(\"\\n--- English: Scarecrow & Award ---\")\n",
    "print(generate_joke(\"Write a joke containing the following words: scarecrow, award\"))\n",
    "\n",
    "print(\"\\n--- English: Tomato & Ketchup ---\")\n",
    "print(generate_joke(\"Write a joke containing the following words: tomato, ketchup\"))\n",
    "\n",
    "print(\"\\n--- English: Pirate & Alphabet ---\")\n",
    "print(generate_joke(\"Write a joke containing the following words: pirate, alphabet\"))\n",
    "\n",
    "print(\"\\n--- English: Chef & Salt ---\")\n",
    "print(generate_joke(\"Write a joke containing the following words: chef, salt\"))\n",
    "\n",
    "print(\"\\n--- English: Computer & Window ---\")\n",
    "print(generate_joke(\"Write a joke containing the following words: computer, window\"))\n",
    "\n",
    "print(\"\\n--- English: Elevator & Songs ---\")\n",
    "print(generate_joke(\"Write a joke containing the following words: elevator, songs\"))\n",
    "\n",
    "print(\"\\n--- English: Library & Loud ---\")\n",
    "print(generate_joke(\"Write a joke containing the following words: library, loud\"))\n",
    "\n",
    "print(\"\\n--- English: Gym & Pizza ---\")\n",
    "print(generate_joke(\"Write a joke containing the following words: gym, pizza\"))\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# SPANISH WORD PAIRS\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n--- Spanish: Moon & Cheese (Luna y Queso) ---\")\n",
    "print(generate_joke(\"Escribe un chiste que contenga las siguientes palabras: luna, queso\"))\n",
    "\n",
    "print(\"\\n--- Spanish: Clock & Time (Reloj y Tiempo) ---\")\n",
    "print(generate_joke(\"Escribe un chiste que contenga las siguientes palabras: reloj, tiempo\"))\n",
    "\n",
    "print(\"\\n--- Spanish: Horse & Chair (Caballo y Silla) ---\")\n",
    "print(generate_joke(\"Escribe un chiste que contenga las siguientes palabras: caballo, silla\"))\n",
    "\n",
    "print(\"\\n--- Spanish: Shoe & Stone (Zapato y Piedra) ---\")\n",
    "print(generate_joke(\"Escribe un chiste que contenga las siguientes palabras: zapato, piedra\"))\n",
    "\n",
    "print(\"\\n--- Spanish: Teacher & Exam (Maestro y Examen) ---\")\n",
    "print(generate_joke(\"Escribe un chiste que contenga las siguientes palabras: maestro, examen\"))\n",
    "\n",
    "print(\"\\n--- Spanish: Drunk & Street (Borracho y Calle) ---\")\n",
    "print(generate_joke(\"Escribe un chiste que contenga las siguientes palabras: borracho, calle\"))\n",
    "\n",
    "print(\"\\n--- Spanish: Tomato & Road (Tomate y Carretera) ---\")\n",
    "print(generate_joke(\"Escribe un chiste que contenga las siguientes palabras: tomate, carretera\"))\n",
    "\n",
    "print(\"\\n--- Spanish: Book & WiFi (Libro y WiFi) ---\")\n",
    "print(generate_joke(\"Escribe un chiste que contenga las siguientes palabras: libro, wifi\"))\n",
    "\n",
    "print(\"\\n--- Spanish: Bird & Cage (Pájaro y Jaula) ---\")\n",
    "print(generate_joke(\"Escribe un chiste que contenga las siguientes palabras: pájaro, jaula\"))\n",
    "\n",
    "print(\"\\n--- Spanish: Doctor & Apple (Médico y Manzana) ---\")\n",
    "print(generate_joke(\"Escribe un chiste que contenga las siguientes palabras: médico, manzana\"))\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# CHINESE WORD PAIRS\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n--- Chinese: Programmer & Hair (程序员 & 头发) ---\")\n",
    "print(generate_joke(\"讲一个包含以下词语的笑话：程序员，头发\"))\n",
    "\n",
    "print(\"\\n--- Chinese: Mosquito & Sleep (蚊子 & 睡觉) ---\")\n",
    "print(generate_joke(\"讲一个包含以下词语的笑话：蚊子，睡觉\"))\n",
    "\n",
    "print(\"\\n--- Chinese: Husband & Wallet (老公 & 钱包) ---\")\n",
    "print(generate_joke(\"讲一个包含以下词语的笑话：老公，钱包\"))\n",
    "\n",
    "print(\"\\n--- Chinese: Subway & Crowded (地铁 & 拥挤) ---\")\n",
    "print(generate_joke(\"讲一个包含以下词语的笑话：地铁，拥挤\"))\n",
    "\n",
    "print(\"\\n--- Chinese: Beef & Lamppost (牛肉 & 电线杆) ---\")\n",
    "print(generate_joke(\"讲一个包含以下词语的笑话：牛肉，电线杆\"))\n",
    "\n",
    "print(\"\\n--- Chinese: Student & Homework (学生 & 作业) ---\")\n",
    "print(generate_joke(\"讲一个包含以下词语的笑话：学生，作业\"))\n",
    "\n",
    "print(\"\\n--- Chinese: Fish & Water (鱼 & 水) ---\")\n",
    "print(generate_joke(\"讲一个包含以下词语的笑话：鱼，水\"))\n",
    "\n",
    "print(\"\\n--- Chinese: Clock & Late (闹钟 & 迟到) ---\")\n",
    "print(generate_joke(\"讲一个包含以下词语的笑话：闹钟，迟到\"))\n",
    "\n",
    "print(\"\\n--- Chinese: Driver & Police (司机 & 警察) ---\")\n",
    "print(generate_joke(\"讲一个包含以下词语的笑话：司机，警察\"))\n",
    "\n",
    "print(\"\\n--- Chinese: Money & Happiness (钱 & 快乐) ---\")\n",
    "print(generate_joke(\"讲一个包含以下词语的笑话：钱，快乐\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hx5IoZiBbo5I"
   },
   "source": [
    "Download the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "GB7TxtZpbtT8",
    "outputId": "facf1aee-25d0-4cd8-9073-5828ea21b0a9"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_0435ed04-fedd-497a-b5fd-9cb20f62c18c\", \"qwen3_jokes_local.zip\", 302761643)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "zip_name = \"/content/qwen3_jokes_local\"\n",
    "# Zip the folder from Drive\n",
    "shutil.make_archive(zip_name, 'zip', drive_output_dir)\n",
    "\n",
    "# Download the zip to your computer\n",
    "files.download(zip_name + \".zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "erNZM_-yMySa"
   },
   "outputs": [],
   "source": [
    "from google.colab import runtime\n",
    "runtime.unassign()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
